# Support for Managing the Storage attached to the node.

Node-bot is a infrastructure container (or a daemonset) that runs on each of the Kuberentes node. Node-bot is intended to be extensible and is to act as a bridge to augment the functionality already provided by the kubelet or other infrastructure containers running on the node. For example, node-bot will have to interface with node-problem-detector or heapster etc., 

This is a design proposal is specificially for managing the storage attached to a Kubernetes node by a node-bot.  

## Background

Container Attached Storages like OpenEBS require a greater control in terms of accessing and managing the underlying storage infrastructure like SSDs, NVMe devices, etc., OpenEBS requires the following functionality to be available within a Kubernetes Cluster:
- Need to know about the different types of disks available in the Cluster
- Need to determine the physical location of the disks in the cluster in terms of Node, Rack, DataCenter/Zone
- Need to associate the required storage to OpenEBS Storage Pods, taking into consideration the capacity, qos and availability requirements 
- Need immediate notifications on the faults on the storage disks and ability to perform online corrections

## Possible Solutions

### Using Local PV

Within the Kubernetes, the closest construct that is available to represent a disk is a Persistent Volume. To provide the above functionality with PVs:
- PV objects need to be created statically by discovering all the hardware that is attached to a given node. The approach of the Local PV Provisioner could be used here. (TODO Kiran - Add the link to the Static Local PV Provisioner, that requires the disks to be available in a certain directory and creates PVs with each of those disks.). Modify/extend the provisioner to add the following additional information:
  * PVs will be associated with node,rack information that will be obtained by the labels in the nodes. (TODO Kiran - Find the information on how nodes are associated with rack/zone information)
  * PVs will get additional information regarding the type of the disk, iops that is capable of etc., 
- OpenEBS Provisioner could then query for the PVs using `kubectl get pv` (filtered with local-provisioner type) to get information on all the disks in the Cluster.
- OpenEBS Provisioner can then create OpenEBS Storage Pods that make use of the PV. 
- Have the Node-Bot monitor for faults on the PVs and send notification to the OpenEBS Storage Pods using them. The monitoring includes inforamtion like:
  * gathering smart statistics of the disk
  * probing or listening for events generated by the underlying disk

However with this approach of using PV:
- Once Storage Pod is running, any of the following operations will require a Storage Pod Restart (TODO Kiran - link the issue here):
  * new PVs have to attached to expand the storage available
  * Remove or replace a failed PV

### Using New CRD called PersistentDisk 
- The new CRD called PD will be modelled against PV and will have much of the information available there. 
- Node-bot will create PDs found on the node where it is running and will associate the information like type, location details - node, rack, zone, etc., 
- OpenEBS Provisioner could then query for the PVs using `kubectl get pd` to get information on all the disks in the Cluster.
- OpenEBS Provisioner can then create OpenEBS Storage Pods that are mounted with location where devices can be found say `/dev/` in a Previleged mode. The actual disks to be used will be passed using a CRD (say cStorPool) 
- Have the Node-Bot monitor for faults on the PDs and send notification to the OpenEBS Storage Pods using them. The monitoring includes inforamtion like:
  * gathering smart statistics of the disk
  * probing or listening for events generated by the underlying disk
- OpenEBS Provisioner can update the cStorPool with the new PDs for expansion or replacement

Drawback/Issues:
- The underlying disks used/allocaed to OpenEBS Storage Pods should not be assigned by Kuberentes (say via Local Provisioner) to different workloads/pods.

