# Create NDM configmap
# Define the Service Account
# Define the RBAC rules for the Service Account
# Launch the node-disk-manager ( daemon set )

# Create NDM configmap
apiVersion: v1
kind: ConfigMap
metadata:
  name: node-disk-manager-config
  namespace: default
data:
  # node-disk-manager-config contains config of available probes and filters.
  # Probes and Filters will initialize with default values if config for that
  # filter or probe are not present in configmap

  # udev-probe is default or primary probe it should be enabled to run ndm
  # filterconfigs contails configs of filters. To provide a group of include
  # and exclude values add it as , separated string
  node-disk-manager.config: |
    probeconfigs:
      - key: udev-probe
        name: udev probe
        state: true
      - key: seachest-probe
        name: seachest probe
        state: true
      - key: smart-probe
        name: smart probe
        state: true
    filterconfigs:
      - key: os-disk-exclude-filter
        name: os disk exclude filter
        state: true
        exclude: "/,/etc/hosts,/boot"
      - key: vendor-filter
        name: vendor filter
        state: true
        include: ""
        exclude: "CLOUDBYT,OpenEBS"
      - key: path-filter
        name: path filter
        state: true
        include: ""
        exclude: loop

---
# Create NDM Service Account
apiVersion: v1
kind: ServiceAccount
metadata:
  name: openebs-ndm-operator
  namespace: default
---
# Define Role that allows operations on K8s pods/deployments
#  in "default" namespace
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  namespace: default
  name: openebs-ndm-operator
rules:
- apiGroups: ["*"]
  resources: ["disks", "blockdevices", "blockdeviceclaims", "pods", "services", "endpoints", "events", "configmaps", "secrets", "jobs"]
  verbs:
  - '*'
- apiGroups:
  - openebs.io
  resources:
  - '*'
  - blockdevices
  - blockdeviceclaims
  verbs:
  - '*'
---
# Bind the Service Account with the Role Privileges.
# TODO: Check if default account also needs to be there
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: openebs-ndm-operator
  namespace: default
subjects:
- kind: ServiceAccount
  name: openebs-ndm-operator
  namespace: default
- kind: User
  name: system:serviceaccount:default:default
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: openebs-ndm-operator
  apiGroup: rbac.authorization.k8s.io
---
apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
  name: disks.openebs.io
spec:
  group: openebs.io
  names:
    kind: Disk
    listKind: DiskList
    plural: disks
    singular: disk
    shortNames:
    - disk
  version: v1alpha1
  scope: Cluster
---
apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
  name: blockdevices.openebs.io
spec:
  group: openebs.io
  names:
    kind: BlockDevice
    listKind: BlockDeviceList
    plural: blockdevices
    singular: blockdevice
    shortNames:
    - bd
  scope: Namespaced
  version: v1alpha1
---
apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
  name: blockdeviceclaims.openebs.io
spec:
  group: openebs.io
  names:
    kind: BlockDeviceClaim
    listKind: BlockDeviceClaimList
    plural: blockdeviceclaims
    singular: blockdeviceclaim
    shortNames:
    - bdc
  scope: Namespaced
  version: v1alpha1
---
apiVersion: extensions/v1beta1
kind: DaemonSet
metadata:
  name: node-disk-manager
spec:
  updateStrategy:
    type: RollingUpdate
  selector:
    matchLabels:
      name: node-disk-manager
  template:
    metadata:
      labels:
        name: node-disk-manager
    spec:
      # By default the node-disk-manager will be run on all kubernetes nodes
      # If you would like to limit this to only some nodes, say the nodes
      # that have storage attached, you could label those node and use nodeSelector.
      # Example: Label the storage nodes with - "openebs.io/nodegroup"="storage-node"
      # kubectl label node <node-name> "openebs.io/nodegroup"="storage-node"
      # nodeSelector:
      #   "openebs.io/nodegroup": "storage-node"
      # Use host network as container network to monitor udev source using netlink
      # to detect disk attach and detach events using fd.
      hostNetwork: true
      serviceAccountName: openebs-ndm-operator
      containers:
      - name: node-disk-manager
        image: openebs/node-disk-manager-amd64:ci
        imagePullPolicy: Always
        securityContext:
          privileged: true
        # make udev database available inside container
        volumeMounts:
        - name: config
          mountPath: /host/node-disk-manager.config
          subPath: node-disk-manager.config
          readOnly: true
        - name: udev
          mountPath: /run/udev
        - name: procmount
          mountPath: /host/proc
          readOnly: true
        - name: sparsepath
          mountPath: /var/openebs/sparse
        env:
        # namespace in which NDM is installed will be passed to NDM Daemonset
        # as environment variable
        - name: NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        # pass hostname as env variable using downward API to the NDM container
        - name: NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        # specify the directory where the sparse files need to be created.
        # if not specified, then sparse files will not be created.
        - name: SPARSE_FILE_DIR
          value: "/var/openebs/sparse"
        # Size of the sparse file to be created.
        - name: SPARSE_FILE_SIZE
          value: "1073741824"
        # Specify the number of sparse files to be created
        - name: SPARSE_FILE_COUNT
          value: "1"
      volumes:
      - name: config
        configMap:
          name: node-disk-manager-config
      - name: udev
        hostPath:
          path: /run/udev
          type: Directory
      - name: procmount
      # mount /proc/1/mounts (mount file of process 1 of host) inside container
      # to read which partition is mounted on / path
        hostPath:
          path: /proc
          type: Directory
      - name: sparsepath
        hostPath:
          path: /var/openebs/sparse
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: node-disk-operator
spec:
  replicas: 1
  selector:
    matchLabels:
      name: node-disk-operator
  template:
    metadata:
      labels:
        name: node-disk-operator
    spec:
      serviceAccountName: openebs-ndm-operator
      containers:
        - name: node-disk-operator
          # Replace this with the built image name
          image: openebs/node-disk-operator-amd64:ci
          ports:
          - containerPort: 8080
            name: liveness
          command:
          - ndo
          imagePullPolicy: Always
          readinessProbe:
            exec:
              command:
                - stat
                - /tmp/operator-sdk-ready
            initialDelaySeconds: 4
            periodSeconds: 10
            failureThreshold: 1
          env:
            - name: WATCH_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: OPERATOR_NAME
              value: "node-disk-operator"
            - name: CLEANUP_JOB_IMAGE
              value: "quay.io/openebs/linux-utils:3.9"
