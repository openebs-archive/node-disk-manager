# Create NDM configmap
# Define the Service Account
# Define the RBAC rules for the Service Account
# Launch the node-disk-manager ( daemon set )

# Create NDM configmap
apiVersion: v1
kind: ConfigMap
metadata:
  name: node-disk-manager-config
  namespace: default
data:
  # node-disk-manager-config contains config of available probes and filters.
  # Probes and Filters will initialize with default values if config for that
  # filter or probe are not present in configmap

  # udev-probe is default or primary probe it should be enabled to run ndm
  # filterconfigs contails configs of filters. To provide a group of include
  # and exclude values add it as , separated string
  node-disk-manager.config: |
    data:
      localdiskmodels:
        - EphemeralDisk
        - Virtual_disk
    probeconfigs:
      - key: udev-probe
        name: udev probe
        state: true
      - key: seachest-probe
        name: seachest probe
        state: true
      - key: smart-probe
        name: smart probe
        state: true
    filterconfigs:
      - key: os-disk-exclude-filter
        name: os disk exclude filter
        state: true
        exclude: "/,/etc/hosts,/boot"
      - key: vendor-filter
        name: vendor filter
        state: true
        include: ""
        exclude: "CLOUDBYT,OpenEBS"
      - key: path-filter
        name: path filter
        state: true
        include: ""
        exclude: loop
---
# Create NDM Service Account
apiVersion: v1
kind: ServiceAccount
metadata:
  name: openebs-ndm-operator
  namespace: default
---
# Define Role that allows operations on K8s pods/deployments
#  in "default" namespace
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  namespace: default
  name: openebs-ndm-operator
rules:
- apiGroups: ["*"]
  resources: ["disks"]
  verbs: ["*"]
---
# Bind the Service Account with the Role Privileges.
# TODO: Check if default account also needs to be there
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: openebs-ndm-operator
  namespace: default
subjects:
- kind: ServiceAccount
  name: openebs-ndm-operator
  namespace: default
- kind: User
  name: system:serviceaccount:default:default
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: openebs-ndm-operator
  apiGroup: rbac.authorization.k8s.io
---
apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
  # name must match the spec fields below, and be in the form: <plural>.<group>
  name: disks.openebs.io
spec:
  # group name to use for REST API: /apis/<group>/<version>
  group: openebs.io
  # version name to use for REST API: /apis/<group>/<version>
  version: v1alpha1
  scope: Cluster
  names:
    # plural name to be used in the URL: /apis/<group>/<version>/<plural>
    plural: disks
    # singular name to be used as an alias on the CLI and for display
    singular: disk
    # kind is normally the CamelCased singular type. Your resource manifests use this.
    kind: Disk
    # shortNames allow shorter string to match your resource on the CLI
    shortNames:
    - disk
---
apiVersion: extensions/v1beta1
kind: DaemonSet
metadata:
  name: node-disk-manager
spec:
  template:
    metadata:
      labels:
        app: node-disk-manager
    spec:
      # By default the node-disk-manager will be run on all kubernetes nodes
      # If you would like to limit this to only some nodes, say the nodes
      # that have storage attached, you could label those node and use nodeSelector.
      # Example: Label the storage nodes with - "openebs.io/nodegroup"="storage-node"
      # kubectl label node <node-name> "openebs.io/nodegroup"="storage-node"
      # nodeSelector:
      #   "openebs.io/nodegroup": "storage-node"
      # Use host network as container network to monitor udev source using netlink
      # to detect disk attach and detach events using fd.
      hostNetwork: true
      serviceAccountName: openebs-ndm-operator
      containers:
      - name: node-disk-manager
        image: openebs/node-disk-manager-amd64:ci
        imagePullPolicy: Always
        securityContext:
          privileged: true
        # make udev database available inside container
        volumeMounts:
        - name: config
          mountPath: /host/node-disk-manager.config
          subPath: node-disk-manager.config
          readOnly: true
        - name: udev
          mountPath: /run/udev
        - name: procmount
          mountPath: /host/mounts
        - name: sparsepath
          mountPath: /var/openebs
        env:
        # pass hostname as env variable using downward API to the NDM container
        - name: NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        # specify the directory where the sparse files need to be created.
        # if not specified, then sparse files will not be created.
        - name: SPARSE_FILE_DIR
          value: "/var/openebs"
        # Size of the sparse file to be created.
        - name: SPARSE_FILE_SIZE
          value: "1073741824"
        # Specify the number of sparse files to be created
        - name: SPARSE_FILE_COUNT
          value: "1"
      volumes:
      - name: config
        configMap:
          name: node-disk-manager-config
      - name: udev
        hostPath:
          path: /run/udev
          type: Directory
      - name: procmount
      # mount /proc/1/mounts (mount file of process 1 of host) inside container
      # to read which partition is mounted on / path
        hostPath:
          path: /proc/1/mounts
      - name: sparsepath
        hostPath:
          path: /var/openebs
