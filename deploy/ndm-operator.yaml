# This manifest is autogenerated via 'make manifests' command.
# Do the modification to the yamls in deploy/yamls/  directory
# and then run 'make manifests' command

# This manifest deploys the OpenEBS NDM components with associated RBAC rules.

# Create the OpenEBS namespace
apiVersion: v1
kind: Namespace
metadata:
  name: openebs
---
# Create NDM configmap
apiVersion: v1
kind: ConfigMap
metadata:
  name: node-disk-manager-config
  namespace: openebs
data:
  # node-disk-manager-config contains config of available probes and filters.
  # Probes and Filters will initialize with default values if config for that
  # filter or probe are not present in configmap

  # udev-probe is default or primary probe it should be enabled to run ndm
  # filterconfigs contains configs of filters. To provide a group of include
  # and exclude values add it as , separated string
  node-disk-manager.config: |
    probeconfigs:
      - key: udev-probe
        name: udev probe
        state: true
      - key: seachest-probe
        name: seachest probe
        state: true
      - key: smart-probe
        name: smart probe
        state: true
    filterconfigs:
      - key: os-disk-exclude-filter
        name: os disk exclude filter
        state: true
        exclude: "/,/etc/hosts,/boot"
      - key: vendor-filter
        name: vendor filter
        state: true
        include: ""
        exclude: "CLOUDBYT,OpenEBS"
      - key: path-filter
        name: path filter
        state: true
        include: ""
        exclude: "/dev/loop,/dev/fd0,/dev/sr0,/dev/ram,/dev/md,/dev/dm-,/dev/rbd,/dev/zd"
---
# Create NDM Service Account
apiVersion: v1
kind: ServiceAccount
metadata:
  name: openebs-maya-operator
  namespace: openebs
---
# Define Role that allows operations on K8s pods/deployments
# in "openebs" namespace
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  namespace: openebs
  name: openebs-ndm-operator
rules:
- apiGroups: ["*"]
  resources: ["nodes", "pods", "services", "endpoints", "events", "configmaps", "secrets", "jobs"]
  verbs:
  - '*'
- apiGroups: ["apiextensions.k8s.io"]
  resources: ["customresourcedefinitions"]
  verbs:
  - '*'
- apiGroups:
  - openebs.io
  resources:
  - blockdevices
  - blockdeviceclaims
  verbs:
  - '*'
---
# Bind the Service Account with the Role Privileges.
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: openebs-ndm-operator
  namespace: openebs
subjects:
- kind: ServiceAccount
  name: openebs-maya-operator
  namespace: openebs
roleRef:
  kind: ClusterRole
  name: openebs-ndm-operator
  apiGroup: rbac.authorization.k8s.io
---
# Create NDM daemonset
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: node-disk-manager
  namespace: openebs
spec:
  updateStrategy:
    type: RollingUpdate
  selector:
    matchLabels:
      name: node-disk-manager
  template:
    metadata:
      labels:
        name: node-disk-manager
    spec:
      # By default the node-disk-manager will be run on all kubernetes nodes
      # If you would like to limit this to only some nodes, say the nodes
      # that have storage attached, you could label those node and use nodeSelector.
      # Example: Label the storage nodes with - "openebs.io/nodegroup"="storage-node"
      # kubectl label node <node-name> "openebs.io/nodegroup"="storage-node"
      # nodeSelector:
      #   "openebs.io/nodegroup": "storage-node"
      # Use host network as container network to monitor udev source using netlink
      # to detect disk attach and detach events using fd.
      hostNetwork: true
      hostPID: true
      serviceAccountName: openebs-maya-operator
      containers:
      - name: node-disk-manager
        image: openebs/node-disk-manager:ci
        args:
        - -v=4
        - --feature-gates="GPTBasedUUID"
        - --feature-gates="APIService"
        # Default address is 0.0.0.0:9115, do not use quotes around the address
        # - --api-service-address=0.0.0.0:9115
        - --feature-gates="UseOSDisk"
        imagePullPolicy: IfNotPresent
        securityContext:
          privileged: true
        volumeMounts:
        - name: config
          mountPath: /host/node-disk-manager.config
          subPath: node-disk-manager.config
          readOnly: true
          # make udev database available inside container
        - name: udev
          mountPath: /run/udev
        - name: procmount
          mountPath: /host/proc
          readOnly: true
        - name: devmount
          mountPath: /dev
        - name: basepath
          mountPath: /var/openebs/ndm
        - name: sparsepath
          mountPath: /var/openebs/sparse
        env:
        # namespace in which NDM is installed will be passed to NDM Daemonset
        # as environment variable
        - name: NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        # pass hostname as env variable using downward API to the NDM container
        - name: NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        # specify the directory where the sparse files need to be created.
        # if not specified, then sparse files will not be created.
        - name: SPARSE_FILE_DIR
          value: "/var/openebs/sparse"
        # Size of the sparse file to be created.
        - name: SPARSE_FILE_SIZE
          value: "1073741824"
        # Specify the number of sparse files to be created
        - name: SPARSE_FILE_COUNT
          value: "0"
        # Set the core dump env to enable core dump for NDM daemon
        #- name: ENABLE_COREDUMP
        #  value: "1"
      volumes:
      - name: config
        configMap:
          name: node-disk-manager-config
      - name: udev
        hostPath:
          path: /run/udev
          type: Directory
      - name: procmount
        # mount /proc/1/mounts (mount file of process 1 of host) inside container
        # to read which partition is mounted on / path
        hostPath:
          path: /proc
          type: Directory
      - name: devmount
        # the /dev directory is mounted so that we have access to the devices that
        # are connected at runtime of the pod.
        hostPath:
          path: /dev
          type: Directory
      - name: basepath
        hostPath:
          path: /var/openebs/ndm
          type: DirectoryOrCreate
      - name: sparsepath
        hostPath:
          path: /var/openebs/sparse
---
# Create NDM operator deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: node-disk-operator
  namespace: openebs
spec:
  replicas: 1
  strategy:
    type: Recreate
  selector:
    matchLabels:
      name: node-disk-operator
  template:
    metadata:
      labels:
        name: node-disk-operator
    spec:
      serviceAccountName: openebs-maya-operator
      containers:
      - name: node-disk-operator
        image: openebs/node-disk-operator:ci
        ports:
        - containerPort: 8080
          name: liveness
        imagePullPolicy: IfNotPresent
        readinessProbe:
          exec:
            command:
            - stat
            - /tmp/operator-sdk-ready
          initialDelaySeconds: 4
          periodSeconds: 10
          failureThreshold: 1
        env:
        - name: WATCH_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        # the service account of this pod
        - name: SERVICE_ACCOUNT
          valueFrom:
            fieldRef:
              fieldPath: spec.serviceAccountName
        - name: OPERATOR_NAME
          value: "node-disk-operator"
        - name: CLEANUP_JOB_IMAGE
          value: "openebs/linux-utils:ci"
       # OPENEBS_IO_IMAGE_PULL_SECRETS environment variable is used to pass the image pull secrets
       # to the cleanup pod launched by NDM operator
       #- name: OPENEBS_IO_IMAGE_PULL_SECRETS
       #  value: ""
---
# Create NDM cluster exporter deployment.
# This is an optional component and is not required for the basic
# functioning of NDM
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ndm-cluster-exporter
  namespace: openebs
spec:
  replicas: 1
  strategy:
    type: Recreate
  selector:
    matchLabels:
      name: ndm-cluster-exporter
  template:
    metadata:
      labels:
        name: ndm-cluster-exporter
    spec:
      serviceAccountName: openebs-maya-operator
      containers:
      - name: ndm-cluster-exporter
        image: openebs/node-disk-exporter:ci
        command:
        - /usr/local/bin/exporter
        args:
        - "start"
        - "--mode=cluster"
        - "--port=:9100"
        - "--metrics=/metrics"
        ports:
        - containerPort: 9100
          protocol: TCP
          name: metrics
        imagePullPolicy: IfNotPresent
        env:
        - name: NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
---
# Create NDM node exporter daemonset.
# This is an optional component used for getting disk level
# metrics from each of the storage nodes
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: ndm-node-exporter
  namespace: openebs
spec:
  updateStrategy:
    type: RollingUpdate
  selector:
    matchLabels:
      name: ndm-node-exporter
  template:
    metadata:
      labels:
        name: ndm-node-exporter
    spec:
      serviceAccountName: openebs-maya-operator
      containers:
      - name: node-disk-exporter
        image: openebs/node-disk-exporter:ci
        command:
        - /usr/local/bin/exporter
        args:
        - "start"
        - "--mode=node"
        - "--port=:9101"
        - "--metrics=/metrics"
        ports:
        - containerPort: 9101
          protocol: TCP
          name: metrics
        imagePullPolicy: IfNotPresent
        securityContext:
          privileged: true
        env:
        - name: NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
---
